%-------------------------------------------------------------------------------
% REPLACE WITH YOUR PUBLICATIONS
%-------------------------------------------------------------------------------
@article{yang2022cloudprofiler,
  title={Cloudprofiler: TSC-based inter-node profiling and high-throughput data ingestion for cloud streaming workloads},
  author={Yang, Shinhyung and Jeong, Jiun and Scholz, Bernhard and Burgstaller, Bernd},
  journal={arXiv preprint arXiv:2205.09325},
  year={2022}
}

@misc{yang2023cloudprofiler,
  title={Cloudprofiler: TSC-Based Inter-Node Profiling and High-Throughput Data Ingestion for Cloud Streaming Workloads}, 
  author={Shinhyung Yang and Jiun Jeong and Bernhard Scholz and Bernd Burgstaller},
  year={2023},
  eprint={2205.09325},
  archivePrefix={arXiv},
  primaryClass={cs.DC},
  url={https://arxiv.org/abs/2205.09325},
  note={arXiv preprint, arXiv:2205.09325},
  doi={10.48550/arXiv.2205.09325},
}

%2024 한국컴퓨터종합학술대회
%워크스테이션 클러스터에서 스케일러블 이더리움 스마트 컨트랙트 오프 더 체인 테스팅
In Proceedings of Korea Computer Congress 2024 (KCC 2024)
%title     = {Scalable off-the-chain testing of Ethereum smart contracts on a cluster of workstations}, in the paper
@inproceedings{jeong2024workstation,
  author    = {Jiun Jeong and Yeonsoo Kim and Seongho Jeong and Bernd Burgstaller},
  title     = {Scalable Off-The-Chain Testing of Ethereum Smart Contracts on a Cluster of Workstations},
  booktitle = {Proceedings of the Korea Computer Congress 2024 (KCC 2024)},
  url={https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11862288},
  pages={1624--1626},
  year={2024},
  note      = {June 26-28, 2024},
  location  = {ICC Jeju, Jeju Island, Republic of Korea},
}
%
  %eventdate = {June 26-28, 2024},
  %제주국제컨벤션센터(ICC 제주)
2024년 6월 26일(수)~28일(금)
@inproceedings{IEE:2009,
  author    = {Taekhoon Kim and
               Jiin Park and
               Hwangho Kim and
               Jongtae Park and
               Yousun Ko and
               Kirak Hong and
               Bernd Burgstaller},
  title     = {Enhancing WIPI-C with a fast scripting engine for mobile game development},
  booktitle = {Institute of Embedded Engineering of Korea, Jeju Island, Korea},
  year      = {2009},
}


@article{nam2024,
  author = {Hyunwoo Nam and Jay Hwan Lee and Shinhyung Yang and Yeonsoo Kim and Jiun Jeong and Jeonggeun Kim and Bernd Burgstaller},
  
  title = {Comprehensive Design Space Exploration for Graph Neural Network Aggregation on GPUs},

  note = {Submitted to IEEE Computer Architecture Letters, October 1, 2024.},
}
%journal={IEEE Computer Architecture Letters},
%  year={2024},

@article{10876606,
  author={Nam, Hyunwoo and Lee, Jay Hwan and Yang, Shinhyung and Kim, Yeonsoo and Jeong, Jiun and Kim, Jeonggeun and Burgstaller, Bernd},
  journal={IEEE Computer Architecture Letters}, 
  title={Comprehensive Design Space Exploration for Graph Neural Network Aggregation on GPUs}, 
  year={2025},
  volume={24},
  number={1},
  pages={45-48},
  abstract={Graph neural networks (GNNs) have become the state-of-the-art technology for extracting and predicting data representations on graphs. With increasing demand to accelerate GNN computations, the GPU has become the dominant platform for GNN training and inference. GNNs consist of a compute-bound combination phase and a memory-bound aggregation phase. The memory access patterns of the aggregation phase remain a major performance bottleneck on GPUs, despite recent microarchitectural enhancements. Although GNN characterizations have been conducted to investigate this bottleneck, they did not reveal the impact of architectural modifications. However, a comprehensive understanding of improvements from such modifications is imperative to devise GPU optimizations for the aggregation phase. In this letter, we explore the GPU design space for aggregation by assessing the performance improvement potential of a series of architectural modifications. We find that the low locality of aggregation deteriorates performance with increased thread-level parallelism, and a significant enhancement follows memory access optimizations, which remain effective even with software optimization. Our analysis provides insights for hardware optimizations to significantly improve GNN aggregation on GPUs.},
  keywords={Random access memory;Graphics processing units;Bandwidth;Vectors;Software;Optimization;Graph neural networks;Microarchitecture;Training;Computer science;Graph neural networks;graphics processing units;sensitivity analysis},
  doi={10.1109/LCA.2025.3539371},
  ISSN={1556-6064},
  month={Jan},}
